{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "#you will have to pip install extra_keras_datasets\n",
    "from extra_keras_datasets import emnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading dataset = emnist\n",
      "WARNING:root:Please cite the following paper when using or referencing this Extra Keras Dataset:\n",
      "WARNING:root:Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n"
     ]
    }
   ],
   "source": [
    "#byclass does uppercase, lowercase, and digits\n",
    "(x_train, y_train), (x_test, y_test) = emnist.load_data(type = 'byclass')\n",
    "\n",
    "num_train = len(x_train) // 3\n",
    "num_test = len(x_test) // 2\n",
    "\n",
    "valid_labels = list(range(10,62))\n",
    "train_mask = np.isin(y_train, valid_labels)\n",
    "test_mask = np.isin(y_test, valid_labels)\n",
    "\n",
    "x_train = x_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "x_test = x_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "\n",
    "x_train, y_train = x_train[:num_train], y_train[:num_train]\n",
    "x_test, y_test = x_test[:num_test], y_test[:num_test]\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        #self.dropout1 = Dropout(0.25)\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01))\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        #self.dropout2 = Dropout(0.5)\n",
    "        #10 in digits, 62 in byclass\n",
    "        self.d2 = Dense(62, activation='softmax',kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6326807141304016, Accuracy: 79.58168029785156, Test Loss: 0.49696922302246094, Test Accuracy: 83.11067962646484\n",
      "Epoch 2, Loss: 0.45093977451324463, Accuracy: 84.04386138916016, Test Loss: 0.47372791171073914, Test Accuracy: 83.85343933105469\n",
      "Epoch 3, Loss: 0.3904116153717041, Accuracy: 85.59300994873047, Test Loss: 0.4783876836299896, Test Accuracy: 83.6024169921875\n",
      "Epoch 4, Loss: 0.3349608778953552, Accuracy: 87.22039031982422, Test Loss: 0.5198637247085571, Test Accuracy: 83.17085266113281\n",
      "Epoch 5, Loss: 0.28489384055137634, Accuracy: 88.88430786132812, Test Loss: 0.5798599720001221, Test Accuracy: 82.29397583007812\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LETTERMODEL\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./LETTERMODEL\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, './LETTERMODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('/Users/jackl/Documents/AIPRAC/enigma-ocr/Test Data/1.png')\n",
    "#This resizes the image to the size it needs to be\n",
    "image = image.resize((28, 28), resample=Image.BILINEAR)\n",
    "image = np.asarray(image)\n",
    "image = color.rgb2gray(image)\n",
    "\n",
    "#dimension stuff\n",
    "image = np.expand_dims(image, axis=-1)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "y_pred = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predDict(num):\n",
    "  df = pd.read_excel(\"./Dictionary.xlsx\", sheet_name=0)\n",
    "  data_dict = dict(zip(df['Numbers'], df['Symbols']))\n",
    "  return data_dict[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class = np.argmax(y_pred, axis=-1)\n",
    "predDict(y_pred_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22a519fdbc8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.squeeze(image),cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
